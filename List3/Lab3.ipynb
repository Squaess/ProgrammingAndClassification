{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21. For a given bitstring __b__ list all bitstrings **b’**, such that the Hamming distance between `b` and `b’` is equal 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitstring_hamming_distance(b):\n",
    "    n = len(bin(b)) - 2\n",
    "    pattern = 2**(n-1)\n",
    "    while pattern > 0:\n",
    "        print(format(b ^ pattern, f\"0{n}b\"))\n",
    "        pattern = pattern // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00101\n",
      "11101\n",
      "10001\n",
      "10111\n",
      "10100\n"
     ]
    }
   ],
   "source": [
    "bitstring = 0b10101\n",
    "bitstring_hamming_distance(bitstring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22. Construct a function that returns a Jaccard similarity for two sets. Beware that this function needs to check if at least one of the sets is nonempty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_sim(s1, s2):\n",
    "    assert s1 or s2 , \"One of the set must contain elements\"\n",
    "    return (len(set(s1) & set(s2)) / len(set(s1) | set(s2)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.2\n"
     ]
    }
   ],
   "source": [
    "a = jaccard_sim({2,5}, {5,2})\n",
    "b = jaccard_sim({1,2,3}, {1,4,5})\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23. Construct a function that computes Jaccard similarity for two strings treated as bags of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def jacc2(s1:str, s2:str) -> float:\n",
    "    s1 = s1.translate(str.maketrans(string.punctuation, \" \"*len(string.punctuation)))\n",
    "    s2 = s2.translate(str.maketrans(string.punctuation, \" \"*len(string.punctuation)))\n",
    "    splited1 = [word.lower() for word in s1.split()]\n",
    "    splited2 = [word.lower() for word in s2.split()]\n",
    "    freq1 = {i:splited1.count(i) for i in set(splited1)}\n",
    "    freq2 = {i:splited2.count(i) for i in set(splited2)}\n",
    "    inter = 0\n",
    "    for i in set(splited1) & set(splited2):\n",
    "        inter += min(freq1[i], freq2[i])\n",
    "    \n",
    "    return inter/(len(splited1) + len(splited2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jacc2(\"Witam Pana Michała.\", \"Witam PaNa witolda i Pana Michała.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24. (use NLTK) List all words in `text1` with edit distance from the word `dog` smaller than 4. Hint: you can safely reject all long words without computations (why?)\n",
    "\n",
    "We know that $L(v, w) \\ge ||v| - |w||$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', ']', '.', '(', ')'] ['among', 'along', 'dogged', 'dogger', 'dodges']\n",
      "['[', 'Moby', 'by', ']', '.']\n",
      "69100\n",
      "148344\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "l1 = [token for token in list(filter(lambda x: len(x) < 7 and x not in stop_words, text1))\n",
    "         if nltk.edit_distance(token, \"dog\") < 4]\n",
    "l2 = [token for token in list(filter(lambda x: len(x) < 7, text1))\n",
    "         if nltk.edit_distance(token, \"dog\") < 4]\n",
    "l1 = sorted(l1, key=len)\n",
    "print(l1[:5], l1[-5:])\n",
    "print(l2[:5])\n",
    "print(len(l1))\n",
    "print(len(l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25. (use NLTK) Let `text1-text9` be bags of words. Compute similarity between all pairs of texts. \n",
    "Can't compute jacard distance on baggs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22789809919305756"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# for t1, t2 in combinations([text1, text2, text3, text4, text5, text6, text7, text8, text9], 2):\n",
    "#     print(f\"{t1.name} - {t2.name}: {nltk.jaccard_distance(set(filter(lambda x: x not in stop_words,t1)), set(filter(lambda x: x not in stop_words, t2)))}\")\n",
    "\n",
    "def jac_bags_nltk(t1, t2):\n",
    "    \n",
    "    t1 = list(filter(lambda x: x not in stop_words, t1))\n",
    "    t2 = list(filter(lambda x: x not in stop_words, t2))\n",
    "    freq1 = {i:t1.count(i) for i in set(t1)}\n",
    "    freq2 = {i:t2.count(i) for i in set(t2)}\n",
    "    inter = 0\n",
    "    for i in set(t1) & set(t2):\n",
    "        inter += min(freq1[i], freq2[i])\n",
    "        \n",
    "    return inter/(len(t1) + len(t2))\n",
    "\n",
    "\n",
    "# for t1, t2 in combinations([text1, text2, text3, text4, text5, text6, text7, text8, text9], 2):\n",
    "#     print(t1.name, t2.name)\n",
    "    \n",
    "jac_bags_nltk(text1, text2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26. (use NLTK) Let us consider a metric space $(S, d)$, where $S$ is the set of words from `text1` and $d$ is the Hamming distance. Find diameter of $(S, d)$.\n",
    "\n",
    "A metric space $M = (\\mathcal{A}, d)$ is called bounded if there exists some number $r$, such that $(\\forall x,y\\in\\mathcal{A}) d(x,y) \\le r$. The smallest possible such $r$ is called the diameter of $M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def find_diameter(text):\n",
    "    d = defaultdict(list)\n",
    "    for i in set([t.lower() for t in text]):\n",
    "        d[len(i)].append(i)\n",
    "\n",
    "    d = sorted(d.items(), reverse=True)\n",
    "\n",
    "    r = (0, \"\", \"\")\n",
    "    control = False\n",
    "    for i in range(len(d)):\n",
    "        word_length = d[i][0]\n",
    "        value = d[i][1]\n",
    "        for s1, s2 in combinations(value,2):\n",
    "            dist = sum([ex != ey for ex, ey in zip(s1, s2)])\n",
    "            r = (dist, s1, s2) if dist > r[0] else r\n",
    "        if i+1 < len(d) and r[0] >= d[i+1][0]:\n",
    "            break\n",
    "            \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(17, 'cannibalistically', 'preternaturalness'),\n",
       " (16, 'disqualifications', 'companionableness'),\n",
       " (14, 'interpretations', 'zaphnathpaaneah'),\n",
       " (17, 'antiphilosophists', 'contradistinction'),\n",
       " (31, ')))))))))))))))))))))))))))))))', 'wooooooooooooohoooooooooooooooo')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(find_diameter, [text1, text2, text3, text4, text5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 27. (use NLTK) Construct a dictionary that assigns each pair of consecutive words in `text1` the Jaccard similarity between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('her', 'missing'), 0.0),\n",
       " (('missing', 'children'), 0.18181818181818177),\n",
       " (('found', 'another'), 0.19999999999999996),\n",
       " (('another', 'orphan'), 0.625),\n",
       " (('orphan', '.'), 0.0)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zad27 = {(t1,t2):(1-nltk.jaccard_distance(set(t1), set(t2))) for t1, t2 in zip(text1, text1[1:])}\n",
    "list(zad27.items())[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28. (use NLTK). For two words $v$ and $w$, let `relative edit distance` be the Levensthein distance between $v$ and $w$ divided by the sum of lengths $v$ and $w$. Find two different words in `text2` with minimal relative edit distance.\n",
    "\n",
    "We know that $\\mid   \\mid v \\mid - \\mid w \\mid \\mid \\leqslant L(v, w) \\leqslant \\max\\{\\mid v \\mid, \\mid w \\mid\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "\n",
    "def find_closest(text):\n",
    "    \n",
    "    def relative_edit(a, b):\n",
    "        return nltk.edit_distance(a, b) / (len(a) + len(b))\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    d = defaultdict(set)\n",
    "\n",
    "    for i in text:\n",
    "        if i.lower() not in stop_words:\n",
    "            d[len(i)].add(i.lower())\n",
    "        \n",
    "    d = sorted(d.items(), reverse=True)\n",
    "    \n",
    "    def possibly_min(w1:int, w2:int) -> float:\n",
    "        \"\"\" Return possibly minimall relative edit distance\n",
    "        \"\"\"\n",
    "        return 1/(w1 + w2) if w1 == w2 else abs(w1 - w2)/(w1 + w2)\n",
    "\n",
    "    def checker(index:int, min_i:float) -> int:\n",
    "        \"\"\" Return index from what it is worth to check\n",
    "\n",
    "        We should return index of the elements in list where start to compering\n",
    "        \"\"\"\n",
    "        i = index\n",
    "        if i >= len(d):\n",
    "            return index\n",
    "        for j in range(i+1):\n",
    "            c_min = possibly_min(d[j][0], d[i][0])\n",
    "            if c_min < min_i:\n",
    "                return j\n",
    "        return -1\n",
    "\n",
    "    min_val = 1\n",
    "    min_words = []\n",
    "\n",
    "    for i in range(len(d)):\n",
    "        # get index from what to search\n",
    "        idx = checker(i, min_val)\n",
    "        if idx == -1:\n",
    "            # we can't do better going further\n",
    "            break\n",
    "\n",
    "        for j in range(idx, i+1):\n",
    "            ll = [(relative_edit(w1, w2), w1, w2) for w1 in d[j][1] for w2 in d[i][1] if j != i]\n",
    "            if j == i:\n",
    "                ll = [(relative_edit(w1,w2), w1, w2) for w1, w2 in combinations(d[i][1], 2)]\n",
    "\n",
    "            m_l, w1, w2 = min(\n",
    "                ll,\n",
    "                default=(1, \"\", \"\"), \n",
    "                key=lambda x: x[0]\n",
    "            )\n",
    "            if m_l < min_val:\n",
    "                min_val = m_l\n",
    "                min_words = [(w1, w2) for v, w1, w2 in ll if v == m_l]\n",
    "            elif m_l == min_val:\n",
    "                min_words.extend([(w1, w2) for v, w1, w2 in ll if v == m_l])\n",
    "\n",
    "\n",
    "    return (min_val, min_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.030303030303030304, [('circumnavigations', 'circumnavigation')])\n",
      "(0.034482758620689655, [('representations', 'representation'), ('disappointments', 'disappointment'), ('acknowledgments', 'acknowledgment')])\n",
      "(0.034482758620689655, [('interpretations', 'interpretation')])\n",
      "(0.034482758620689655, [('accomplishments', 'accomplishment'), ('recommendations', 'recommendation'), ('representations', 'representation'), ('discriminations', 'discrimination'), ('administrations', 'administration'), ('congratulations', 'congratulation'), ('representatives', 'representative')])\n",
      "(0.01818181818181818, [('!!!!!!!!!!!!!!!!!!!!!!!!!!!!', '!!!!!!!!!!!!!!!!!!!!!!!!!!!')])\n",
      "(0.045454545454545456, [('syndicalist', 'syndicalism')])\n",
      "(0.034482758620689655, [('forest-products', 'forest-product'), ('administrations', 'administration'), ('recommendations', 'recommendation'), ('microprocessors', 'microprocessor'), ('superconductors', 'superconductor'), ('representatives', 'representative'), ('pharmaceuticals', 'pharmaceutical'), ('fundamentalists', 'fundamentalist')])\n",
      "(0.047619047619047616, [('appreciated', 'appreciate')])\n",
      "(0.03571428571428571, [('unquestionably', 'unquestionable')])\n"
     ]
    }
   ],
   "source": [
    "for tup in list(map(lambda x: find_closest(x),[text1, text2, text3, text4, text5, text6, text7, text8, text9])):\n",
    "    print(tup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# d = defaultdict(set)\n",
    "\n",
    "# def relative_edit(a, b):\n",
    "#     return nltk.edit_distance(a, b) / (len(a) + len(b))\n",
    "\n",
    "# for i in text2:\n",
    "#     if i.lower() not in stop_words:\n",
    "#         d[len(i)].add(i.lower())\n",
    "        \n",
    "# d = sorted(d.items(), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-4b19666a3add>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mmin_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;31m# get index from what to search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchecker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "# print(d[0])\n",
    "# print(d[1])\n",
    "# print(d[2])\n",
    "# print(d[3])\n",
    "\n",
    "# def possibly_min(w1:int, w2:int) -> float:\n",
    "#     \"\"\" Return possibly minimall relative edit distance\n",
    "#     \"\"\"\n",
    "#     return 1/(w1 + w2) if w1 == w2 else abs(w1 - w2)/(w1 + w2)\n",
    "\n",
    "# def checker(index:int, min_i:float) -> int:\n",
    "#     \"\"\" Return index from what it is worth to check\n",
    "    \n",
    "#     We should return index of the elements in list where start to compering\n",
    "#     \"\"\"\n",
    "#     i = index\n",
    "#     if i >= len(d):\n",
    "#         return index\n",
    "#     for j in range(i+1):\n",
    "#         c_min = possibly_min(d[j][0], d[i][0])\n",
    "#         if c_min < min_i:\n",
    "#             return j\n",
    "#     return -1\n",
    "    \n",
    "# min_val = 1\n",
    "# min_words = []\n",
    "\n",
    "# for i in range(len(d)):\n",
    "#     # get index from what to search\n",
    "#     idx = checker(i, min_val)\n",
    "#     if idx == -1:\n",
    "#         # we can't do better going further\n",
    "#         break\n",
    "        \n",
    "#     for j in range(idx, i+1):\n",
    "#         ll = [(relative_edit(w1, w2), w1, w2) for w1 in d[j][1] for w2 in d[i][1] if j != i]\n",
    "#         if j == i:\n",
    "#             ll = [(relative_edit(w1,w2), w1, w2) for w1, w2 in combinations(d[i][1], 2)]\n",
    "    \n",
    "#         m_l, w1, w2 = min(\n",
    "#             ll,\n",
    "#             default=(1, \"\", \"\"), \n",
    "#             key=lambda x: x[0]\n",
    "#         )\n",
    "#         if m_l < min_val:\n",
    "#             min_val = m_l\n",
    "#             min_words = [(w1, w2) for v, w1, w2 in ll if v == m_l]\n",
    "#         elif m_l == min_val:\n",
    "#             min_words.extend([(w1, w2) for v, w1, w2 in ll if v == m_l])\n",
    "    \n",
    "\n",
    "# print(min_val, min_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t2 = set(filter(lambda x: x not in stop_words, text2))\n",
    "\n",
    "# def bad():\n",
    "#     m = 1\n",
    "#     s1, s2 = \"\", \"\"\n",
    "#     for d1, d2 in tqdm(combinations(t2, 2)):\n",
    "#         v = relative_edit(d1, d2)\n",
    "#         if v < m:\n",
    "#             print(v, d1, d2)\n",
    "#             m = v\n",
    "#             s1, s2 = d1, d2\n",
    "#     print(m,s1,s2)\n",
    "#     return (m, s1, s2)\n",
    "    \n",
    "# def min_rel():\n",
    "#     for i1, i2 in combinations(d, 2):\n",
    "#         print(i1, i2)\n",
    "#         break\n",
    "#         for s1, s2 in combinations(d, 2):\n",
    "#             dist = relative_edit(s1, s2)\n",
    "#             yield s1, s2, dist\n",
    "            \n",
    "# bad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 29. For a given bitstring **b** and a natural number *r* list all bitstrings __b’__, such that theHamming distance between `b` and `b’` is equal _n_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Return an enumerate object.\n",
       "\n",
       "  iterable\n",
       "    an object supporting iteration\n",
       "\n",
       "The enumerate object yields pairs containing a count (from start, which\n",
       "defaults to zero) and a value yielded by the iterable argument.\n",
       "\n",
       "enumerate is useful for obtaining an indexed list:\n",
       "    (0, seq[0]), (1, seq[1]), (2, seq[2]), ...\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enumerate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "def hamming_distance2(b, r):\n",
    "    assert r <= len(b), \"Hamming distance can't be bigger than length of the given word\"\n",
    "    for c in combinations(enumerate(b), r):\n",
    "        for i in range(len(b)):\n",
    "            if i in [j[0] for j in c]:\n",
    "                print((int(b[i]) + 1) % 2, end= '')\n",
    "            else:\n",
    "                print(b[i], end = '')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11110\n"
     ]
    }
   ],
   "source": [
    "hamming_distance2(\"00001\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30. Construct a function that for a given string and a natural number _k_ returns a **set** of all its *k*-shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_shingles(s, k):\n",
    "    r = set()\n",
    "    for i in range(0, len(s)):\n",
    "        if i+k <= len(s):\n",
    "            r.add(s[i:i+k])\n",
    "        else:\n",
    "            break\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dend', 'eden', 'endw', 'jede', 'ndwa'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_shingles(\"jedendwa\", 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
